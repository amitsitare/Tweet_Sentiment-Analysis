{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label        time                          date     query         username  \\\n",
       "0      0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                text  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the dataset with no columns titles and with latin encoding \n",
    "df_raw = pd.read_csv(r'C:\\Users\\dell\\OneDrive\\Desktop\\twitter\\Sentiment140.csv', encoding='ISO-8859-1', header=None)\n",
    "\n",
    " # As the data has no column titles, we will add our own\n",
    "df_raw.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]\n",
    "\n",
    "# Show the first 5 rows of the dataframe.\n",
    "# You can specify the number of rows to be shown as follows: df_raw.head(10)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    800000\n",
       "4    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the data's output balance\n",
    "# The label '4' denotes positive sentiment and '0' denotes negative sentiment\n",
    "df_raw['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1      0  is upset that he can't update his Facebook by ...\n",
       "2      0  @Kenichan I dived many times for the ball. Man...\n",
       "3      0    my whole body feels itchy and like its on fire \n",
       "4      0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ommiting every column except for the text and the label, as we won't need any of the other information\n",
    "df = df_raw[['label', 'text']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000 800000\n"
     ]
    }
   ],
   "source": [
    "# Seperating positive and negative rows\n",
    "df_pos = df[df['label'] == 4]\n",
    "df_neg = df[df['label'] == 0]\n",
    "print(len(df_pos), len(df_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 200000\n"
     ]
    }
   ],
   "source": [
    "# Only retaining 1/4th of our data from each output group\n",
    "# Feel free to alter the dividing factor depending on your workspace\n",
    "# 1/64 is a good place to start if you're unsure about your machine's power\n",
    "df_pos = df_pos.iloc[:int(len(df_pos)/4)]\n",
    "df_neg = df_neg.iloc[:int(len(df_neg)/4)]\n",
    "print(len(df_pos), len(df_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatinating both positive and negative groups and storing them back into a single dataframe\n",
    "df = pd.concat([df_pos, df_neg])\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleaning and Processing the Data <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Time: 54.20841884613037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['I', 'LOVE', '@Health4UandPets', 'u', 'guys', 'r', 'the', 'best', '!', '!'],\n",
       "  1),\n",
       " (['im',\n",
       "   'meeting',\n",
       "   'up',\n",
       "   'with',\n",
       "   'one',\n",
       "   'of',\n",
       "   'my',\n",
       "   'besties',\n",
       "   'tonight',\n",
       "   '!',\n",
       "   'Cant',\n",
       "   'wait',\n",
       "   '!',\n",
       "   '!',\n",
       "   '-',\n",
       "   'GIRL',\n",
       "   'TALK',\n",
       "   '!',\n",
       "   '!'],\n",
       "  1),\n",
       " (['@DaRealSunisaKim',\n",
       "   'Thanks',\n",
       "   'for',\n",
       "   'the',\n",
       "   'Twitter',\n",
       "   'add',\n",
       "   ',',\n",
       "   'Sunisa',\n",
       "   '!',\n",
       "   'I',\n",
       "   'got',\n",
       "   'to',\n",
       "   'meet',\n",
       "   'you',\n",
       "   'once',\n",
       "   'at',\n",
       "   'a',\n",
       "   'HIN',\n",
       "   'show',\n",
       "   'here',\n",
       "   'in',\n",
       "   'the',\n",
       "   'DC',\n",
       "   'area',\n",
       "   'and',\n",
       "   'you',\n",
       "   'were',\n",
       "   'a',\n",
       "   'sweetheart',\n",
       "   '.'],\n",
       "  1),\n",
       " (['Being',\n",
       "   'sick',\n",
       "   'can',\n",
       "   'be',\n",
       "   'really',\n",
       "   'cheap',\n",
       "   'when',\n",
       "   'it',\n",
       "   'hurts',\n",
       "   'too',\n",
       "   'much',\n",
       "   'to',\n",
       "   'eat',\n",
       "   'real',\n",
       "   'food',\n",
       "   'Plus',\n",
       "   ',',\n",
       "   'your',\n",
       "   'friends',\n",
       "   'make',\n",
       "   'you',\n",
       "   'soup'],\n",
       "  1),\n",
       " (['@LovesBrooklyn2', 'he', 'has', 'that', 'effect', 'on', 'everyone'], 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# The reduce_len parameter will allow a maximum of 3 consecutive repeating characters, while trimming the rest\n",
    "# For example, it will tranform the word: 'Helloooooooooo' to: 'Hellooo'\n",
    "tk = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "data = []\n",
    "\n",
    "# Separating our features (text) and our labels into two lists to smoothen our work\n",
    "X = df['text'].tolist()\n",
    "Y = df['label'].tolist()\n",
    "\n",
    "# Building our data list, that is a list of tuples, where each tuple is a pair of the tokenized text\n",
    "# and its corresponding label\n",
    "for x, y in zip(X, Y):\n",
    "    if y == 4:\n",
    "        data.append((tk.tokenize(x), 1))\n",
    "    else:\n",
    "        data.append((tk.tokenize(x), 0))\n",
    "        \n",
    "# Printing the CPU time and the first 5 elements of our 'data' list\n",
    "print('CPU Time:', time() - start_time)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('LOVE', 'VBP'), ('@Health4UandPets', 'NNS'), ('u', 'JJ'), ('guys', 'NNS'), ('r', 'VBP'), ('the', 'DT'), ('best', 'JJS'), ('!', '.'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Previewing the pos_tag() output\n",
    "print(pos_tag(data[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'LOVE', '@Health4UandPets', 'u', 'guy', 'r', 'the', 'best', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        # First, we will convert the pos_tag output tags to a tag format that the WordNetLemmatizer can interpret\n",
    "        # In general, if a tag starts with NN, the word is a noun and if it stars with VB, the word is a verb.\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "# Previewing the WordNetLemmatizer() output\n",
    "print(lemmatize_sentence(data[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'guy', 'best']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:45: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:45: SyntaxWarning: invalid escape sequence '\\('\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_9564\\3129781592.py:45: SyntaxWarning: invalid escape sequence '\\('\n",
      "  token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    "\n",
    "# Stopwords are frequently-used words (such as “the”, “a”, “an”, “in”) that do not hold any meaning useful to extract sentiment.\n",
    "# If it's your first time ever using nltk, you can download nltk's stopwords using: nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "# A custom function defined in order to fine-tune the cleaning of the input text. This function is highly dependent on each usecase.\n",
    "# Note: Only include misspelling or abbreviations of commonly used words.\n",
    "#       Including many minimally present cases would negatively impact the performance. \n",
    "def cleaned(token):\n",
    "    if token == 'u':\n",
    "        return 'you'\n",
    "    if token == 'r':\n",
    "        return 'are'\n",
    "    if token == 'some1':\n",
    "        return 'someone'\n",
    "    if token == 'yrs':\n",
    "        return 'years'\n",
    "    if token == 'hrs':\n",
    "        return 'hours'\n",
    "    if token == 'mins':\n",
    "        return 'minutes'\n",
    "    if token == 'secs':\n",
    "        return 'seconds'\n",
    "    if token == 'pls' or token == 'plz':\n",
    "        return 'please'\n",
    "    if token == '2morow':\n",
    "        return 'tomorrow'\n",
    "    if token == '2day':\n",
    "        return 'today'\n",
    "    if token == '4got' or token == '4gotten':\n",
    "        return 'forget'\n",
    "    if token == 'amp' or token == 'quot' or token == 'lt' or token == 'gt' or token == '½25':\n",
    "        return ''\n",
    "    return token\n",
    "\n",
    "# This function will be our all-in-one noise removal function\n",
    "def remove_noise(tweet_tokens):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        # Eliminating the token if it is a link\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        # Eliminating the token if it is a mention\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        cleaned_token = cleaned(token.lower())\n",
    "        \n",
    "        # Eliminating the token if its length is less than 3, if it is a punctuation or if it is a stopword\n",
    "        if cleaned_token not in string.punctuation and len(cleaned_token) > 2 and cleaned_token not in STOP_WORDS:\n",
    "            cleaned_tokens.append(cleaned_token)\n",
    "            \n",
    "    return cleaned_tokens\n",
    "\n",
    "# Prevewing the remove_noise() output\n",
    "print(remove_noise(data[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Noise, CPU Time: 579.8559310436249\n",
      "Data Prepared for model, CPU Time: 0.8608980178833008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[({'love': True, 'guy': True, 'best': True}, 1),\n",
       " ({'meet': True,\n",
       "   'one': True,\n",
       "   'besties': True,\n",
       "   'tonight': True,\n",
       "   'cant': True,\n",
       "   'wait': True,\n",
       "   'girl': True,\n",
       "   'talk': True},\n",
       "  1),\n",
       " ({'thanks': True,\n",
       "   'twitter': True,\n",
       "   'add': True,\n",
       "   'sunisa': True,\n",
       "   'get': True,\n",
       "   'meet': True,\n",
       "   'hin': True,\n",
       "   'show': True,\n",
       "   'area': True,\n",
       "   'sweetheart': True},\n",
       "  1),\n",
       " ({'sick': True,\n",
       "   'really': True,\n",
       "   'cheap': True,\n",
       "   'hurt': True,\n",
       "   'much': True,\n",
       "   'eat': True,\n",
       "   'real': True,\n",
       "   'food': True,\n",
       "   'plus': True,\n",
       "   'friend': True,\n",
       "   'make': True,\n",
       "   'soup': True},\n",
       "  1),\n",
       " ({'effect': True, 'everyone': True}, 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "# As the Naive Bayesian classifier accepts inputs in a dict-like structure,\n",
    "# we have to define a function that transforms our data into the required input structure\n",
    "def list_to_dict(cleaned_tokens):\n",
    "    return dict([token, True] for token in cleaned_tokens)\n",
    "\n",
    "cleaned_tokens_list = []\n",
    "\n",
    "# Removing noise from all the data\n",
    "for tokens, label in data:\n",
    "    cleaned_tokens_list.append((remove_noise(tokens), label))\n",
    "\n",
    "print('Removed Noise, CPU Time:', time() - start_time)\n",
    "start_time = time()\n",
    "\n",
    "final_data = []\n",
    "\n",
    "# Transforming the data to fit the input structure of the Naive Bayesian classifier\n",
    "for tokens, label in cleaned_tokens_list:\n",
    "    final_data.append((list_to_dict(tokens), label))\n",
    "    \n",
    "print('Data Prepared for model, CPU Time:', time() - start_time)\n",
    "\n",
    "# Previewing our final (tokenized, cleaned and lemmatized) data list\n",
    "final_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time()\n",
    "\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# positive_words = []\n",
    "# negative_words = []\n",
    "\n",
    "# # Separating out positive and negative words (i.e., words appearing in negative and positive tweets),\n",
    "# # in order to visualize each set of words seperately\n",
    "# for i in range(len(cleaned_tokens_list)):\n",
    "#     if cleaned_tokens_list[i][1] == 1:\n",
    "#         positive_words.extend(cleaned_tokens_list[i][0])\n",
    "#     else:\n",
    "#         negative_words.extend(cleaned_tokens_list[i][0])\n",
    "\n",
    "# # Defining our word cloud drawing function\n",
    "# def wordcloud_draw(data, color = 'black'):\n",
    "#     wordcloud = WordCloud(stopwords = STOPWORDS,\n",
    "#                           background_color = color,\n",
    "#                           width = 2500,\n",
    "#                           height = 2000\n",
    "#                          ).generate(' '.join(data))\n",
    "#     plt.figure(1, figsize = (13, 13))\n",
    "#     plt.imshow(wordcloud)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "    \n",
    "# print(\"Positive words\")\n",
    "# wordcloud_draw(positive_words, 'white')\n",
    "# print(\"Negative words\")\n",
    "# wordcloud_draw(negative_words)        \n",
    "\n",
    "# print('CPU Time:', time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayesian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As our data is currently ordered by label, we have to shuffle it before splitting it\n",
    "# .Random(140) randomizes our data with seed = 140. This guarantees the same shuffling for every execution of our code\n",
    "# Feel free to alter this value or even omit it to have different outputs for each code execution\n",
    "random.Random(140).shuffle(final_data)\n",
    "\n",
    "# Here we decided to split our data as 90% train data and 10% test data\n",
    "# Once again, feel free to alter this number and test the model accuracy\n",
    "trim_index = int(len(final_data) * 0.9)\n",
    "\n",
    "train_data = final_data[:trim_index]\n",
    "test_data = final_data[trim_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train data: 0.8107305555555555\n",
      "Accuracy on test data: 0.755575\n",
      "Most Informative Features\n",
      "               depressed = True                0 : 1      =     49.0 : 1.0\n",
      "               toothache = True                0 : 1      =     45.0 : 1.0\n",
      "                    roni = True                0 : 1      =     34.3 : 1.0\n",
      "                 unhappy = True                0 : 1      =     31.4 : 1.0\n",
      "                   strep = True                0 : 1      =     31.0 : 1.0\n",
      "                  asthma = True                0 : 1      =     26.3 : 1.0\n",
      "                 unloved = True                0 : 1      =     25.0 : 1.0\n",
      "                  #movie = True                1 : 0      =     23.0 : 1.0\n",
      "                  gutted = True                0 : 1      =     22.3 : 1.0\n",
      "                   hates = True                0 : 1      =     21.9 : 1.0\n",
      "               heartburn = True                0 : 1      =     21.7 : 1.0\n",
      "                     bom = True                1 : 0      =     21.4 : 1.0\n",
      "                   ugggh = True                0 : 1      =     19.7 : 1.0\n",
      "              depressing = True                0 : 1      =     19.4 : 1.0\n",
      "                     ftl = True                0 : 1      =     19.0 : 1.0\n",
      "                   waaah = True                0 : 1      =     18.6 : 1.0\n",
      "                coughing = True                0 : 1      =     18.2 : 1.0\n",
      "          congratulation = True                1 : 0      =     18.1 : 1.0\n",
      "                     sad = True                0 : 1      =     17.9 : 1.0\n",
      "         congratulations = True                1 : 0      =     17.5 : 1.0\n",
      "None\n",
      "\n",
      "CPU Time: 22.028595685958862\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "# Output the model accuracy on the train and test data\n",
    "print('Accuracy on train data:', classify.accuracy(classifier, train_data))\n",
    "print('Accuracy on test data:', classify.accuracy(classifier, test_data))\n",
    "\n",
    "# Output the words that provide the most information about the sentiment of a tweet.\n",
    "# These are words that are heavily present in one sentiment group and very rarely present in the other group.\n",
    "print(classifier.show_most_informative_features(20))\n",
    "\n",
    "print('\\nCPU Time:', time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sentiment tweet\n"
     ]
    }
   ],
   "source": [
    "tweet = \"he is very good boy. he is perform very good in the class. he also help his friends\"\n",
    "\n",
    "tweet_tokens = remove_noise(tk.tokenize(tweet))  # Preprocess the input tweet\n",
    "\n",
    "# Classify the preprocessed tweet using the trained classifier\n",
    "if classifier.classify(dict([token, True] for token in tweet_tokens)) == 0:\n",
    "    print(\"Negative sentiment tweet\")\n",
    "else:\n",
    "    print(\"Positive sentiment tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier saved as pickle file: sentiment_classifier.pickle\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Define the file path where you want to save the pickle file\n",
    "pickle_file_path = \"sentiment_classifier.pickle\"\n",
    "\n",
    "# Save the trained classifier to a pickle file\n",
    "with open(pickle_file_path, 'wb') as file:\n",
    "    pickle.dump(classifier, file)\n",
    "\n",
    "print(\"Classifier saved as pickle file:\", pickle_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
